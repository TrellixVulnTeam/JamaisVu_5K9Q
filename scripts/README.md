## Overview
This directory includes the scripts for running Jamais Vu.

Before using these scripts, you need to set environment variables
```GEM5_ROOT=<path to gem5 root>``` and ```WORKLOADS_ROOT=<path to SPEC2017 root>```.
Note that the workload directory must be structured appropriately before using any of the scripts.
Please refer to this [Section](#Structure-of-Workload-Directory)
for more information.

## Methodology
We use the [SimPoint](http://cseweb.ucsd.edu/~calder/simpoint/) methodology
to generate up to 10 representative intervals that accurately characterize
end-to-end performance. Each interval has a gem5 checkpoint that allows
Jamais Vu to resume from.

## Structure of Workload Directory
The SPEC2017 directory has the following structure:
```
SPEC2017
├── ckpt  # SimPoint checkpoints
│   ├── blender_r  # SPEC2017 application name
│   │   ├── cpt.None.SIMP-0  # SimPoint checkpoint for interval 0
│   │   │   ├── m5.cpt  # standard gem5 checkpoint file
│   │   │   └── system.physmem.store0.pmem  # standard gem5 checkpoint file
│   │   │
│   │   ├── cpt.None.SIMP-1  # SimPoint checkpoint for interval 1
│   │   │   ├── m5.cpt
│   │   │   └── system.physmem.store0.pmem
│   │   │
│   │   ├── ...  # more checkpoint dirs
│   │   │
│   │   ├── results.simpts  # SimPoint region information, generated by SimPoint
│   │   └── results.weights  # SimPoint weight information, generated by SimPoint
│   │
│   └── ...  # more SPEC2017 applications
│
└── run  # directories that contain SPEC2017 binaries and input files
    ├── blender_r  # SPEC2017 application name
    │   ├── blender_r  # binary, it should have the same name as the application
    │   ├── blender_r.epochs  # epoch information
    │   └── ...  # more misc files used by the benchmark
    │
    └── ...  # more SPEC2017 applications
```

## Execute from a Single SimPoint Checkpoint
Scripts `unsafe.sh`, `CoR.sh`, `counter.sh`, and `epoch.sh` launch Jamais Vu
from a single SimPoint checkpoint. These four scripts correspond to *Unsafe* baseline,
*Clear-on-Retire* scheme, *Counter* scheme, and *Epoch* scheme.

These scripts take four positional arguments: `benchmark name`, `interval ID`,
`study name`, and `config name`. `study name` and `config name` are strings
that indicate which study the execution belongs to and the configuration used
by the execution.
The simulation output directory will be
```$GEM5_ROOT/output/<study name>/<config name>/<benchmark>/<interval ID>```.
Please refer to this [Document](https://www.gem5.org/documentation/learning_gem5/part1/gem5_stats/)
about understanding gem5 output and stats.

These four scripts share some common optional arguments, such as the total number of
instructions to simulate. Each script also has some unique optional arguments,
please refer to the help information of each script for more details.

## Submit Jobs for the Entire SPEC2017 Suite
We provide a script `submit`, which submits jobs to [HTCondor](https://research.cs.wisc.edu/htcondor/)
for a given study and configuration, and launch execution for every SPEC2017 SimPoint checkpoint.

`submit` takes as inputs a configuration file or a list of configuration files.
A configuration file should look like:
```cfg
NAME=CoR@128        # configuration name
SCRIPT=CoR.sh       # script that runs the experiments
ARGS=--elem-cnt 128 # optional arguments for the script
```
Note that, leaving any whitespace between `=` and the `<value>` may lead to unexpected behaviors.
Also, the study name is implicitly assigned as the name of the directory that contains the configuration.

After jobs are submitted, the script `status` provides check job status for each
study and configuration.

## Reproducibility
To reproduce the results, we created 5 study directories: `perf`, `elemCnt`,
`activeRecord`, `CBFBits`, and `CCGeometry` under `$GEM5_ROOT/scripts`.
Each study corresponds to a figure in the evaluation section of the paper.

The description of each study is the following:
1. `perf`: corresponds to Figure 7 in the paper. It simulates all three schemes
plus unsafe baseline and measures normalized execution time;
2. `elemCnt`: corresponds to Figure 8 in the paper. It performs a sensitivity
study on the number of entries per bloom filter for Clear-on-Retire,
Epoch-Iter-Rem, and Epoch-Loop-Rem;
3. `activeRecord`: corresponds to Figure 9 in the paper.
It performs a sensitivity study on the number of {ID, PC-buffer}
pairs for Epoch-Iter-Rem and Epoch-Loop-Rem;
4. `CBFBits`: corresponds to Figure 10 in the paper.
It performs a sensitivity study on the number of bits per
counting bloom filter entry for Epoch-Iter-Rem and Epoch-Loop-Rem;
5. `CCGeometry`: corresponds to Figure 11 in the paper.
It performs a sensitivity study on the counter cache geometry for Counter scheme.

### Benchmark Suite
We used the SPEC2017rate benchmark suite.
Due to simulation issue with gem5, we exclude 
2 applications (cactuBSSN and imagick) out of 23 from SPEC2017.
We use the [SimPoint](http://cseweb.ucsd.edu/~calder/simpoint/)
methodology to generate up to 10 representative intervals
that accurately characterize end-to-end performance for each application.
We have 155 intervals in total.
Due to copyright issues, we are not be able to share our binaries and checkpoints.
If you want to use your own binaries and checkpoints, please organize SPEC2017
directory according to this [Section](#Assumptions-on-Workload-Directory).

### Job Submission
Before submitting jobs, please make sure environment variables
```GEM5_ROOT=<path to gem5 root>```
and
```WORKLOADS_ROOT=<path to SPEC2017 root>```
are correctly set.
Then execute
```bash
./submit */*.cfg
```
to submit all the jobs.
It will take about 20 minutes to finish job submission.

### Be Patient
There will be ~5700 jobs in total. On average, a job takes ~20 minutes to finish.
So, you can estimate the total execution time based on the number of condor slots on your cluster.
On our environment, which has 80 slots, it takes about 1 day to finish the total number of jobs.
You can use script `status` to print detailed jobs status information.

### Collect Results
After the jobs are finished, you can collect the results using
the scripts named `collect` under each study directory (please make sure that the required
[Python libraries](requirements.txt) are installed). The script will read stats, print stats in CSV
format, and create a plot named `<study name>.pdf`.
The following command will collect results for every study
(don't forget the **backslash and semicolon** at the end):
```bash
find . -name collect -type f -exec {} >/dev/null \;
```

### Expected Results
The collected plots for each study should match its corresponding figure
in the directory `expectedResults`.
